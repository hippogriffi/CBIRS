{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from numpy import linalg\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from pandas import DataFrame\n",
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import operator\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import mahotas as mt\n",
    "\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import img data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'C:/Users/Joe/Desktop/UNI/Yr3/Dissertation/Datasets/101_ObjectCategories'\n",
    "folder_names = []\n",
    "folder_names = [f for f in sorted(os.listdir(file_path))]\n",
    "\n",
    "img_data = []\n",
    "img_classes = []\n",
    "\n",
    "num_categories = 6\n",
    "num_imgs = 9\n",
    "selected_categories = np.random.randint(0, 101, num_categories, dtype=\"I\")\n",
    "\n",
    "for a, cat in enumerate(selected_categories):\n",
    "    folder_path = file_path + '/' + folder_names[cat]\n",
    "    image_names = [a for a in sorted(\n",
    "        os.listdir(folder_path))][:num_imgs]\n",
    "\n",
    "    for b, img_name in enumerate(image_names):\n",
    "        img_classes.append(folder_names[cat])\n",
    "        img_path = folder_path + '/' + img_name\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.resize(img, (100, 100))\n",
    "        \n",
    "\n",
    "        if img is not None:\n",
    "            img_data.append(img)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG16 Model For Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (100, 100, 3)\n",
    "weights = 'imagenet'\n",
    "pooling = 'max'\n",
    "model = VGG16(weights=weights, input_shape=(\n",
    "    input_shape[0], input_shape[1], input_shape[2]), pooling=pooling, include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.expand_dims(img_data[11], axis= 0 ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100, 100, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_feat_extract(img):\n",
    "    img = np.expand_dims(img, axis = 0)\n",
    "    p_img = preprocess_input(img)\n",
    "    feats = model.predict(p_img)\n",
    "    return (feats[0] / np.linalg.norm(feats[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_db_feats(img_db):\n",
    "    db_f_vector = []\n",
    "    for img in img_db:\n",
    "        db_f_vector.append(vgg_feat_extract(img))\n",
    "    return db_f_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n"
     ]
    }
   ],
   "source": [
    "test_db = vgg_db_feats(img_data)\n",
    "stdSlr = StandardScaler().fit(test_db)\n",
    "test_feats = stdSlr.transform(test_db)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearSVC(max_iter=2000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC(max_iter=2000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearSVC(max_iter=2000)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = LinearSVC(max_iter=2000)\n",
    "svm.fit(test_feats, np.array(img_classes))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 79ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[8.56862068e-02 0.00000000e+00 2.66298763e-02 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 6.04993775e-02 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 8.96887407e-02\n 0.00000000e+00 0.00000000e+00 1.05901212e-02 1.79510359e-02\n 0.00000000e+00 3.79775204e-02 0.00000000e+00 2.97780167e-02\n 0.00000000e+00 0.00000000e+00 1.41228251e-02 2.83058714e-02\n 0.00000000e+00 6.34386390e-03 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 3.76471062e-03 0.00000000e+00\n 1.34158945e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 8.71057715e-03 1.81482658e-02 0.00000000e+00 0.00000000e+00\n 8.57634842e-03 0.00000000e+00 3.68218832e-02 5.44693833e-03\n 0.00000000e+00 3.90282134e-03 0.00000000e+00 0.00000000e+00\n 3.50747779e-02 1.35214627e-02 0.00000000e+00 0.00000000e+00\n 1.58481933e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 1.04343764e-01 0.00000000e+00 0.00000000e+00\n 1.85508169e-02 1.10234623e-03 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 2.16734000e-02\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.25751524e-02\n 0.00000000e+00 0.00000000e+00 6.50694501e-03 3.87222506e-02\n 0.00000000e+00 2.92398874e-02 3.63156199e-02 1.14158183e-01\n 7.53366426e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 2.31011193e-02 1.92282423e-02 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 4.69825929e-03 0.00000000e+00\n 2.01252922e-01 0.00000000e+00 4.86435965e-02 2.46699937e-02\n 0.00000000e+00 1.64620299e-02 1.43579543e-02 0.00000000e+00\n 1.71777420e-02 2.69440208e-02 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 1.01453587e-01 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 9.32845171e-04 0.00000000e+00\n 1.47103459e-01 0.00000000e+00 0.00000000e+00 1.45745277e-01\n 1.29103780e-01 4.64640968e-02 2.63506114e-01 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 3.74998078e-02 7.22463802e-02\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 9.98835862e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 5.24106435e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 2.49767248e-02\n 1.57283191e-02 3.86805870e-02 1.19092472e-01 4.27698940e-02\n 3.82307209e-02 1.37664780e-01 0.00000000e+00 3.97768989e-02\n 3.36047113e-02 1.16417423e-01 0.00000000e+00 0.00000000e+00\n 4.33802493e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 1.21528574e-03\n 1.04784826e-02 1.22744940e-01 0.00000000e+00 1.20818205e-01\n 0.00000000e+00 1.86803583e-02 5.64916246e-03 0.00000000e+00\n 0.00000000e+00 1.14455938e-01 2.65812594e-02 0.00000000e+00\n 1.85725652e-02 4.99131009e-02 0.00000000e+00 0.00000000e+00\n 1.29182637e-01 0.00000000e+00 3.38084698e-02 2.95789563e-03\n 0.00000000e+00 2.30057631e-02 3.02939881e-02 0.00000000e+00\n 1.73826180e-02 0.00000000e+00 0.00000000e+00 2.28354167e-02\n 4.18547951e-02 2.12084819e-02 1.01520978e-02 1.25560984e-01\n 1.06636509e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 4.65533398e-02 8.38942826e-03 0.00000000e+00\n 2.22719371e-01 5.21909110e-02 3.07739787e-02 3.48007530e-02\n 1.27969205e-01 0.00000000e+00 0.00000000e+00 2.28271764e-02\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 9.76131633e-02 1.11258499e-01 0.00000000e+00\n 0.00000000e+00 6.59942701e-02 0.00000000e+00 2.54519489e-02\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 1.09223323e-02\n 0.00000000e+00 5.29570580e-02 0.00000000e+00 4.36185254e-03\n 4.50451151e-02 1.00989945e-01 4.96110469e-02 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 9.87222698e-03\n 0.00000000e+00 3.42346281e-02 9.94237289e-02 0.00000000e+00\n 4.47726659e-02 2.66980324e-02 1.85183305e-02 1.54471725e-01\n 8.76166373e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 3.06253079e-02 3.43405046e-02 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 7.41839036e-02 5.30077051e-03 1.26237487e-02\n 1.39451250e-02 1.89719778e-02 0.00000000e+00 0.00000000e+00\n 1.20711364e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 1.54856069e-04\n 2.04798598e-02 4.28344943e-02 1.07696047e-02 2.28410996e-02\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.97052183e-03\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 1.83221363e-02 5.17406734e-03 0.00000000e+00\n 1.13384230e-02 0.00000000e+00 0.00000000e+00 1.07835814e-01\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 3.59939486e-02 0.00000000e+00 4.56281714e-02 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 2.99249459e-02\n 3.04662827e-02 0.00000000e+00 5.50316684e-02 0.00000000e+00\n 1.25769034e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 8.77650548e-03 1.05338909e-01 2.12878697e-02 1.36551633e-02\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 2.52011605e-02\n 1.43873394e-01 3.79036106e-02 0.00000000e+00 5.77996187e-02\n 4.70510451e-03 6.55706152e-02 0.00000000e+00 0.00000000e+00\n 8.35400373e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 5.48978075e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 7.52883404e-03\n 0.00000000e+00 2.13941429e-02 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 6.63382513e-03 9.53249857e-02\n 0.00000000e+00 0.00000000e+00 2.79426556e-02 8.75359476e-02\n 0.00000000e+00 6.63421303e-02 4.50663008e-02 1.25617078e-02\n 4.62954380e-02 0.00000000e+00 2.10419208e-01 1.06808200e-01\n 4.81292419e-02 5.70347952e-03 3.43420245e-02 0.00000000e+00\n 2.71044150e-02 0.00000000e+00 3.49337012e-02 6.89528659e-02\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 1.81057360e-02\n 9.46069788e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 6.42176270e-02 0.00000000e+00 3.23417000e-02 0.00000000e+00\n 9.53117968e-04 0.00000000e+00 0.00000000e+00 2.43683737e-02\n 1.30779177e-01 7.13422596e-02 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 1.77495152e-01 4.02306579e-02 0.00000000e+00 0.00000000e+00\n 1.59360394e-02 8.68743169e-04 1.08872131e-01 0.00000000e+00\n 3.51153277e-02 0.00000000e+00 0.00000000e+00 1.11278214e-01\n 0.00000000e+00 0.00000000e+00 3.09476280e-03 0.00000000e+00\n 0.00000000e+00 2.97704954e-02 0.00000000e+00 7.66669028e-03\n 3.24298665e-02 4.39956561e-02 6.34398907e-02 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 2.98898183e-02 3.80365038e-03\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 8.99919197e-02\n 4.53067496e-02 8.49617459e-03 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 1.09174391e-02 0.00000000e+00 0.00000000e+00\n 1.74786840e-02 9.01329145e-03 0.00000000e+00 0.00000000e+00\n 9.29825939e-03 2.60619577e-02 1.06345778e-02 4.91392380e-03\n 1.06086470e-01 0.00000000e+00 1.77347916e-03 0.00000000e+00\n 0.00000000e+00 1.05241723e-01 0.00000000e+00 0.00000000e+00\n 9.81647670e-02 7.18060788e-03 0.00000000e+00 1.99685469e-02\n 1.04674347e-01 0.00000000e+00 0.00000000e+00 2.08112225e-01\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 7.83631112e-03 0.00000000e+00 0.00000000e+00\n 2.29869746e-02 1.16129611e-02 7.70034939e-02 0.00000000e+00\n 0.00000000e+00 9.80670657e-03 0.00000000e+00 8.31813377e-04\n 0.00000000e+00 3.28001641e-02 7.92734418e-03 0.00000000e+00\n 4.81772274e-02 7.63715804e-02 6.21853098e-02 7.07589090e-02].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [61], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m test22 \u001b[39m=\u001b[39m vgg_feat_extract(img_data[\u001b[39m22\u001b[39m])\n\u001b[1;32m----> 4\u001b[0m test22_res \u001b[39m=\u001b[39m [img_classes[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m svm\u001b[39m.\u001b[39;49mpredict(test22)]\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_base.py:447\u001b[0m, in \u001b[0;36mLinearClassifierMixin.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[0;32m    434\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    435\u001b[0m \u001b[39m    Predict class labels for samples in X.\u001b[39;00m\n\u001b[0;32m    436\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[39m        Vector containing the class labels for each sample.\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 447\u001b[0m     scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecision_function(X)\n\u001b[0;32m    448\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(scores\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    449\u001b[0m         indices \u001b[39m=\u001b[39m (scores \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mastype(\u001b[39mint\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_base.py:429\u001b[0m, in \u001b[0;36mLinearClassifierMixin.decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    410\u001b[0m \u001b[39mPredict confidence scores for samples.\u001b[39;00m\n\u001b[0;32m    411\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[39m    this class would be predicted.\u001b[39;00m\n\u001b[0;32m    426\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    427\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[1;32m--> 429\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    430\u001b[0m scores \u001b[39m=\u001b[39m safe_sparse_dot(X, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoef_\u001b[39m.\u001b[39mT, dense_output\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintercept_\n\u001b[0;32m    431\u001b[0m \u001b[39mreturn\u001b[39;00m scores\u001b[39m.\u001b[39mravel() \u001b[39mif\u001b[39;00m scores\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m scores\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\sklearn\\base.py:577\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    575\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    576\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 577\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    578\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[0;32m    579\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:879\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    877\u001b[0m     \u001b[39m# If input is 1D raise error\u001b[39;00m\n\u001b[0;32m    878\u001b[0m     \u001b[39mif\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 879\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    880\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mExpected 2D array, got 1D array instead:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39marray=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    881\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    882\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mif it contains a single sample.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[0;32m    884\u001b[0m         )\n\u001b[0;32m    886\u001b[0m \u001b[39mif\u001b[39;00m dtype_numeric \u001b[39mand\u001b[39;00m array\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mkind \u001b[39min\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mUSV\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    887\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    888\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdtype=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnumeric\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    889\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    890\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[8.56862068e-02 0.00000000e+00 2.66298763e-02 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 6.04993775e-02 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 8.96887407e-02\n 0.00000000e+00 0.00000000e+00 1.05901212e-02 1.79510359e-02\n 0.00000000e+00 3.79775204e-02 0.00000000e+00 2.97780167e-02\n 0.00000000e+00 0.00000000e+00 1.41228251e-02 2.83058714e-02\n 0.00000000e+00 6.34386390e-03 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 3.76471062e-03 0.00000000e+00\n 1.34158945e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 8.71057715e-03 1.81482658e-02 0.00000000e+00 0.00000000e+00\n 8.57634842e-03 0.00000000e+00 3.68218832e-02 5.44693833e-03\n 0.00000000e+00 3.90282134e-03 0.00000000e+00 0.00000000e+00\n 3.50747779e-02 1.35214627e-02 0.00000000e+00 0.00000000e+00\n 1.58481933e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 1.04343764e-01 0.00000000e+00 0.00000000e+00\n 1.85508169e-02 1.10234623e-03 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 2.16734000e-02\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.25751524e-02\n 0.00000000e+00 0.00000000e+00 6.50694501e-03 3.87222506e-02\n 0.00000000e+00 2.92398874e-02 3.63156199e-02 1.14158183e-01\n 7.53366426e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 2.31011193e-02 1.92282423e-02 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 4.69825929e-03 0.00000000e+00\n 2.01252922e-01 0.00000000e+00 4.86435965e-02 2.46699937e-02\n 0.00000000e+00 1.64620299e-02 1.43579543e-02 0.00000000e+00\n 1.71777420e-02 2.69440208e-02 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 1.01453587e-01 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 9.32845171e-04 0.00000000e+00\n 1.47103459e-01 0.00000000e+00 0.00000000e+00 1.45745277e-01\n 1.29103780e-01 4.64640968e-02 2.63506114e-01 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 3.74998078e-02 7.22463802e-02\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 9.98835862e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 5.24106435e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 2.49767248e-02\n 1.57283191e-02 3.86805870e-02 1.19092472e-01 4.27698940e-02\n 3.82307209e-02 1.37664780e-01 0.00000000e+00 3.97768989e-02\n 3.36047113e-02 1.16417423e-01 0.00000000e+00 0.00000000e+00\n 4.33802493e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 1.21528574e-03\n 1.04784826e-02 1.22744940e-01 0.00000000e+00 1.20818205e-01\n 0.00000000e+00 1.86803583e-02 5.64916246e-03 0.00000000e+00\n 0.00000000e+00 1.14455938e-01 2.65812594e-02 0.00000000e+00\n 1.85725652e-02 4.99131009e-02 0.00000000e+00 0.00000000e+00\n 1.29182637e-01 0.00000000e+00 3.38084698e-02 2.95789563e-03\n 0.00000000e+00 2.30057631e-02 3.02939881e-02 0.00000000e+00\n 1.73826180e-02 0.00000000e+00 0.00000000e+00 2.28354167e-02\n 4.18547951e-02 2.12084819e-02 1.01520978e-02 1.25560984e-01\n 1.06636509e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 4.65533398e-02 8.38942826e-03 0.00000000e+00\n 2.22719371e-01 5.21909110e-02 3.07739787e-02 3.48007530e-02\n 1.27969205e-01 0.00000000e+00 0.00000000e+00 2.28271764e-02\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 9.76131633e-02 1.11258499e-01 0.00000000e+00\n 0.00000000e+00 6.59942701e-02 0.00000000e+00 2.54519489e-02\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 1.09223323e-02\n 0.00000000e+00 5.29570580e-02 0.00000000e+00 4.36185254e-03\n 4.50451151e-02 1.00989945e-01 4.96110469e-02 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 9.87222698e-03\n 0.00000000e+00 3.42346281e-02 9.94237289e-02 0.00000000e+00\n 4.47726659e-02 2.66980324e-02 1.85183305e-02 1.54471725e-01\n 8.76166373e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 3.06253079e-02 3.43405046e-02 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 7.41839036e-02 5.30077051e-03 1.26237487e-02\n 1.39451250e-02 1.89719778e-02 0.00000000e+00 0.00000000e+00\n 1.20711364e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 1.54856069e-04\n 2.04798598e-02 4.28344943e-02 1.07696047e-02 2.28410996e-02\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.97052183e-03\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 1.83221363e-02 5.17406734e-03 0.00000000e+00\n 1.13384230e-02 0.00000000e+00 0.00000000e+00 1.07835814e-01\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 3.59939486e-02 0.00000000e+00 4.56281714e-02 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 2.99249459e-02\n 3.04662827e-02 0.00000000e+00 5.50316684e-02 0.00000000e+00\n 1.25769034e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 8.77650548e-03 1.05338909e-01 2.12878697e-02 1.36551633e-02\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 2.52011605e-02\n 1.43873394e-01 3.79036106e-02 0.00000000e+00 5.77996187e-02\n 4.70510451e-03 6.55706152e-02 0.00000000e+00 0.00000000e+00\n 8.35400373e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 5.48978075e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 7.52883404e-03\n 0.00000000e+00 2.13941429e-02 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 6.63382513e-03 9.53249857e-02\n 0.00000000e+00 0.00000000e+00 2.79426556e-02 8.75359476e-02\n 0.00000000e+00 6.63421303e-02 4.50663008e-02 1.25617078e-02\n 4.62954380e-02 0.00000000e+00 2.10419208e-01 1.06808200e-01\n 4.81292419e-02 5.70347952e-03 3.43420245e-02 0.00000000e+00\n 2.71044150e-02 0.00000000e+00 3.49337012e-02 6.89528659e-02\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 1.81057360e-02\n 9.46069788e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 6.42176270e-02 0.00000000e+00 3.23417000e-02 0.00000000e+00\n 9.53117968e-04 0.00000000e+00 0.00000000e+00 2.43683737e-02\n 1.30779177e-01 7.13422596e-02 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 1.77495152e-01 4.02306579e-02 0.00000000e+00 0.00000000e+00\n 1.59360394e-02 8.68743169e-04 1.08872131e-01 0.00000000e+00\n 3.51153277e-02 0.00000000e+00 0.00000000e+00 1.11278214e-01\n 0.00000000e+00 0.00000000e+00 3.09476280e-03 0.00000000e+00\n 0.00000000e+00 2.97704954e-02 0.00000000e+00 7.66669028e-03\n 3.24298665e-02 4.39956561e-02 6.34398907e-02 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 2.98898183e-02 3.80365038e-03\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 8.99919197e-02\n 4.53067496e-02 8.49617459e-03 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 1.09174391e-02 0.00000000e+00 0.00000000e+00\n 1.74786840e-02 9.01329145e-03 0.00000000e+00 0.00000000e+00\n 9.29825939e-03 2.60619577e-02 1.06345778e-02 4.91392380e-03\n 1.06086470e-01 0.00000000e+00 1.77347916e-03 0.00000000e+00\n 0.00000000e+00 1.05241723e-01 0.00000000e+00 0.00000000e+00\n 9.81647670e-02 7.18060788e-03 0.00000000e+00 1.99685469e-02\n 1.04674347e-01 0.00000000e+00 0.00000000e+00 2.08112225e-01\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 7.83631112e-03 0.00000000e+00 0.00000000e+00\n 2.29869746e-02 1.16129611e-02 7.70034939e-02 0.00000000e+00\n 0.00000000e+00 9.80670657e-03 0.00000000e+00 8.31813377e-04\n 0.00000000e+00 3.28001641e-02 7.92734418e-03 0.00000000e+00\n 4.81772274e-02 7.63715804e-02 6.21853098e-02 7.07589090e-02].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "test22 = vgg_feat_extract(img_data[22])\n",
    "\n",
    "\n",
    "test22_res = [img_classes[i] for i in svm.predict(test22)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[8.56862068e-02 0.00000000e+00 2.66298763e-02 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 6.04993775e-02 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 8.96887407e-02\n 0.00000000e+00 0.00000000e+00 1.05901212e-02 1.79510359e-02\n 0.00000000e+00 3.79775204e-02 0.00000000e+00 2.97780167e-02\n 0.00000000e+00 0.00000000e+00 1.41228251e-02 2.83058714e-02\n 0.00000000e+00 6.34386390e-03 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 3.76471062e-03 0.00000000e+00\n 1.34158945e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 8.71057715e-03 1.81482658e-02 0.00000000e+00 0.00000000e+00\n 8.57634842e-03 0.00000000e+00 3.68218832e-02 5.44693833e-03\n 0.00000000e+00 3.90282134e-03 0.00000000e+00 0.00000000e+00\n 3.50747779e-02 1.35214627e-02 0.00000000e+00 0.00000000e+00\n 1.58481933e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 1.04343764e-01 0.00000000e+00 0.00000000e+00\n 1.85508169e-02 1.10234623e-03 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 2.16734000e-02\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.25751524e-02\n 0.00000000e+00 0.00000000e+00 6.50694501e-03 3.87222506e-02\n 0.00000000e+00 2.92398874e-02 3.63156199e-02 1.14158183e-01\n 7.53366426e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 2.31011193e-02 1.92282423e-02 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 4.69825929e-03 0.00000000e+00\n 2.01252922e-01 0.00000000e+00 4.86435965e-02 2.46699937e-02\n 0.00000000e+00 1.64620299e-02 1.43579543e-02 0.00000000e+00\n 1.71777420e-02 2.69440208e-02 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 1.01453587e-01 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 9.32845171e-04 0.00000000e+00\n 1.47103459e-01 0.00000000e+00 0.00000000e+00 1.45745277e-01\n 1.29103780e-01 4.64640968e-02 2.63506114e-01 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 3.74998078e-02 7.22463802e-02\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 9.98835862e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 5.24106435e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 2.49767248e-02\n 1.57283191e-02 3.86805870e-02 1.19092472e-01 4.27698940e-02\n 3.82307209e-02 1.37664780e-01 0.00000000e+00 3.97768989e-02\n 3.36047113e-02 1.16417423e-01 0.00000000e+00 0.00000000e+00\n 4.33802493e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 1.21528574e-03\n 1.04784826e-02 1.22744940e-01 0.00000000e+00 1.20818205e-01\n 0.00000000e+00 1.86803583e-02 5.64916246e-03 0.00000000e+00\n 0.00000000e+00 1.14455938e-01 2.65812594e-02 0.00000000e+00\n 1.85725652e-02 4.99131009e-02 0.00000000e+00 0.00000000e+00\n 1.29182637e-01 0.00000000e+00 3.38084698e-02 2.95789563e-03\n 0.00000000e+00 2.30057631e-02 3.02939881e-02 0.00000000e+00\n 1.73826180e-02 0.00000000e+00 0.00000000e+00 2.28354167e-02\n 4.18547951e-02 2.12084819e-02 1.01520978e-02 1.25560984e-01\n 1.06636509e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 4.65533398e-02 8.38942826e-03 0.00000000e+00\n 2.22719371e-01 5.21909110e-02 3.07739787e-02 3.48007530e-02\n 1.27969205e-01 0.00000000e+00 0.00000000e+00 2.28271764e-02\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 9.76131633e-02 1.11258499e-01 0.00000000e+00\n 0.00000000e+00 6.59942701e-02 0.00000000e+00 2.54519489e-02\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 1.09223323e-02\n 0.00000000e+00 5.29570580e-02 0.00000000e+00 4.36185254e-03\n 4.50451151e-02 1.00989945e-01 4.96110469e-02 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 9.87222698e-03\n 0.00000000e+00 3.42346281e-02 9.94237289e-02 0.00000000e+00\n 4.47726659e-02 2.66980324e-02 1.85183305e-02 1.54471725e-01\n 8.76166373e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 3.06253079e-02 3.43405046e-02 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 7.41839036e-02 5.30077051e-03 1.26237487e-02\n 1.39451250e-02 1.89719778e-02 0.00000000e+00 0.00000000e+00\n 1.20711364e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 1.54856069e-04\n 2.04798598e-02 4.28344943e-02 1.07696047e-02 2.28410996e-02\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.97052183e-03\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 1.83221363e-02 5.17406734e-03 0.00000000e+00\n 1.13384230e-02 0.00000000e+00 0.00000000e+00 1.07835814e-01\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 3.59939486e-02 0.00000000e+00 4.56281714e-02 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 2.99249459e-02\n 3.04662827e-02 0.00000000e+00 5.50316684e-02 0.00000000e+00\n 1.25769034e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 8.77650548e-03 1.05338909e-01 2.12878697e-02 1.36551633e-02\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 2.52011605e-02\n 1.43873394e-01 3.79036106e-02 0.00000000e+00 5.77996187e-02\n 4.70510451e-03 6.55706152e-02 0.00000000e+00 0.00000000e+00\n 8.35400373e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 5.48978075e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 7.52883404e-03\n 0.00000000e+00 2.13941429e-02 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 6.63382513e-03 9.53249857e-02\n 0.00000000e+00 0.00000000e+00 2.79426556e-02 8.75359476e-02\n 0.00000000e+00 6.63421303e-02 4.50663008e-02 1.25617078e-02\n 4.62954380e-02 0.00000000e+00 2.10419208e-01 1.06808200e-01\n 4.81292419e-02 5.70347952e-03 3.43420245e-02 0.00000000e+00\n 2.71044150e-02 0.00000000e+00 3.49337012e-02 6.89528659e-02\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 1.81057360e-02\n 9.46069788e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 6.42176270e-02 0.00000000e+00 3.23417000e-02 0.00000000e+00\n 9.53117968e-04 0.00000000e+00 0.00000000e+00 2.43683737e-02\n 1.30779177e-01 7.13422596e-02 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 1.77495152e-01 4.02306579e-02 0.00000000e+00 0.00000000e+00\n 1.59360394e-02 8.68743169e-04 1.08872131e-01 0.00000000e+00\n 3.51153277e-02 0.00000000e+00 0.00000000e+00 1.11278214e-01\n 0.00000000e+00 0.00000000e+00 3.09476280e-03 0.00000000e+00\n 0.00000000e+00 2.97704954e-02 0.00000000e+00 7.66669028e-03\n 3.24298665e-02 4.39956561e-02 6.34398907e-02 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 2.98898183e-02 3.80365038e-03\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 8.99919197e-02\n 4.53067496e-02 8.49617459e-03 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 1.09174391e-02 0.00000000e+00 0.00000000e+00\n 1.74786840e-02 9.01329145e-03 0.00000000e+00 0.00000000e+00\n 9.29825939e-03 2.60619577e-02 1.06345778e-02 4.91392380e-03\n 1.06086470e-01 0.00000000e+00 1.77347916e-03 0.00000000e+00\n 0.00000000e+00 1.05241723e-01 0.00000000e+00 0.00000000e+00\n 9.81647670e-02 7.18060788e-03 0.00000000e+00 1.99685469e-02\n 1.04674347e-01 0.00000000e+00 0.00000000e+00 2.08112225e-01\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 7.83631112e-03 0.00000000e+00 0.00000000e+00\n 2.29869746e-02 1.16129611e-02 7.70034939e-02 0.00000000e+00\n 0.00000000e+00 9.80670657e-03 0.00000000e+00 8.31813377e-04\n 0.00000000e+00 3.28001641e-02 7.92734418e-03 0.00000000e+00\n 4.81772274e-02 7.63715804e-02 6.21853098e-02 7.07589090e-02].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [66], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test22 \u001b[39m=\u001b[39m stdSlr\u001b[39m.\u001b[39;49mtransform(test22)\n\u001b[0;32m      2\u001b[0m test22 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mreshape(test22, (\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[0;32m      3\u001b[0m test22\u001b[39m.\u001b[39mshape\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:975\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m    972\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[0;32m    974\u001b[0m copy \u001b[39m=\u001b[39m copy \u001b[39mif\u001b[39;00m copy \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy\n\u001b[1;32m--> 975\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    976\u001b[0m     X,\n\u001b[0;32m    977\u001b[0m     reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    978\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    979\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m    980\u001b[0m     dtype\u001b[39m=\u001b[39;49mFLOAT_DTYPES,\n\u001b[0;32m    981\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    982\u001b[0m )\n\u001b[0;32m    984\u001b[0m \u001b[39mif\u001b[39;00m sparse\u001b[39m.\u001b[39missparse(X):\n\u001b[0;32m    985\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwith_mean:\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\sklearn\\base.py:577\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    575\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    576\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 577\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    578\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[0;32m    579\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:879\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    877\u001b[0m     \u001b[39m# If input is 1D raise error\u001b[39;00m\n\u001b[0;32m    878\u001b[0m     \u001b[39mif\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 879\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    880\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mExpected 2D array, got 1D array instead:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39marray=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    881\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    882\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mif it contains a single sample.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[0;32m    884\u001b[0m         )\n\u001b[0;32m    886\u001b[0m \u001b[39mif\u001b[39;00m dtype_numeric \u001b[39mand\u001b[39;00m array\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mkind \u001b[39min\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mUSV\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    887\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    888\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdtype=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnumeric\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    889\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    890\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[8.56862068e-02 0.00000000e+00 2.66298763e-02 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 6.04993775e-02 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 8.96887407e-02\n 0.00000000e+00 0.00000000e+00 1.05901212e-02 1.79510359e-02\n 0.00000000e+00 3.79775204e-02 0.00000000e+00 2.97780167e-02\n 0.00000000e+00 0.00000000e+00 1.41228251e-02 2.83058714e-02\n 0.00000000e+00 6.34386390e-03 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 3.76471062e-03 0.00000000e+00\n 1.34158945e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 8.71057715e-03 1.81482658e-02 0.00000000e+00 0.00000000e+00\n 8.57634842e-03 0.00000000e+00 3.68218832e-02 5.44693833e-03\n 0.00000000e+00 3.90282134e-03 0.00000000e+00 0.00000000e+00\n 3.50747779e-02 1.35214627e-02 0.00000000e+00 0.00000000e+00\n 1.58481933e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 1.04343764e-01 0.00000000e+00 0.00000000e+00\n 1.85508169e-02 1.10234623e-03 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 2.16734000e-02\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.25751524e-02\n 0.00000000e+00 0.00000000e+00 6.50694501e-03 3.87222506e-02\n 0.00000000e+00 2.92398874e-02 3.63156199e-02 1.14158183e-01\n 7.53366426e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 2.31011193e-02 1.92282423e-02 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 4.69825929e-03 0.00000000e+00\n 2.01252922e-01 0.00000000e+00 4.86435965e-02 2.46699937e-02\n 0.00000000e+00 1.64620299e-02 1.43579543e-02 0.00000000e+00\n 1.71777420e-02 2.69440208e-02 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 1.01453587e-01 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 9.32845171e-04 0.00000000e+00\n 1.47103459e-01 0.00000000e+00 0.00000000e+00 1.45745277e-01\n 1.29103780e-01 4.64640968e-02 2.63506114e-01 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 3.74998078e-02 7.22463802e-02\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 9.98835862e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 5.24106435e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 2.49767248e-02\n 1.57283191e-02 3.86805870e-02 1.19092472e-01 4.27698940e-02\n 3.82307209e-02 1.37664780e-01 0.00000000e+00 3.97768989e-02\n 3.36047113e-02 1.16417423e-01 0.00000000e+00 0.00000000e+00\n 4.33802493e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 1.21528574e-03\n 1.04784826e-02 1.22744940e-01 0.00000000e+00 1.20818205e-01\n 0.00000000e+00 1.86803583e-02 5.64916246e-03 0.00000000e+00\n 0.00000000e+00 1.14455938e-01 2.65812594e-02 0.00000000e+00\n 1.85725652e-02 4.99131009e-02 0.00000000e+00 0.00000000e+00\n 1.29182637e-01 0.00000000e+00 3.38084698e-02 2.95789563e-03\n 0.00000000e+00 2.30057631e-02 3.02939881e-02 0.00000000e+00\n 1.73826180e-02 0.00000000e+00 0.00000000e+00 2.28354167e-02\n 4.18547951e-02 2.12084819e-02 1.01520978e-02 1.25560984e-01\n 1.06636509e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 4.65533398e-02 8.38942826e-03 0.00000000e+00\n 2.22719371e-01 5.21909110e-02 3.07739787e-02 3.48007530e-02\n 1.27969205e-01 0.00000000e+00 0.00000000e+00 2.28271764e-02\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 9.76131633e-02 1.11258499e-01 0.00000000e+00\n 0.00000000e+00 6.59942701e-02 0.00000000e+00 2.54519489e-02\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 1.09223323e-02\n 0.00000000e+00 5.29570580e-02 0.00000000e+00 4.36185254e-03\n 4.50451151e-02 1.00989945e-01 4.96110469e-02 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 9.87222698e-03\n 0.00000000e+00 3.42346281e-02 9.94237289e-02 0.00000000e+00\n 4.47726659e-02 2.66980324e-02 1.85183305e-02 1.54471725e-01\n 8.76166373e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 3.06253079e-02 3.43405046e-02 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 7.41839036e-02 5.30077051e-03 1.26237487e-02\n 1.39451250e-02 1.89719778e-02 0.00000000e+00 0.00000000e+00\n 1.20711364e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 1.54856069e-04\n 2.04798598e-02 4.28344943e-02 1.07696047e-02 2.28410996e-02\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.97052183e-03\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 1.83221363e-02 5.17406734e-03 0.00000000e+00\n 1.13384230e-02 0.00000000e+00 0.00000000e+00 1.07835814e-01\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 3.59939486e-02 0.00000000e+00 4.56281714e-02 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 2.99249459e-02\n 3.04662827e-02 0.00000000e+00 5.50316684e-02 0.00000000e+00\n 1.25769034e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 8.77650548e-03 1.05338909e-01 2.12878697e-02 1.36551633e-02\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 2.52011605e-02\n 1.43873394e-01 3.79036106e-02 0.00000000e+00 5.77996187e-02\n 4.70510451e-03 6.55706152e-02 0.00000000e+00 0.00000000e+00\n 8.35400373e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 5.48978075e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 7.52883404e-03\n 0.00000000e+00 2.13941429e-02 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 6.63382513e-03 9.53249857e-02\n 0.00000000e+00 0.00000000e+00 2.79426556e-02 8.75359476e-02\n 0.00000000e+00 6.63421303e-02 4.50663008e-02 1.25617078e-02\n 4.62954380e-02 0.00000000e+00 2.10419208e-01 1.06808200e-01\n 4.81292419e-02 5.70347952e-03 3.43420245e-02 0.00000000e+00\n 2.71044150e-02 0.00000000e+00 3.49337012e-02 6.89528659e-02\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 1.81057360e-02\n 9.46069788e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 6.42176270e-02 0.00000000e+00 3.23417000e-02 0.00000000e+00\n 9.53117968e-04 0.00000000e+00 0.00000000e+00 2.43683737e-02\n 1.30779177e-01 7.13422596e-02 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 1.77495152e-01 4.02306579e-02 0.00000000e+00 0.00000000e+00\n 1.59360394e-02 8.68743169e-04 1.08872131e-01 0.00000000e+00\n 3.51153277e-02 0.00000000e+00 0.00000000e+00 1.11278214e-01\n 0.00000000e+00 0.00000000e+00 3.09476280e-03 0.00000000e+00\n 0.00000000e+00 2.97704954e-02 0.00000000e+00 7.66669028e-03\n 3.24298665e-02 4.39956561e-02 6.34398907e-02 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 2.98898183e-02 3.80365038e-03\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 8.99919197e-02\n 4.53067496e-02 8.49617459e-03 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 1.09174391e-02 0.00000000e+00 0.00000000e+00\n 1.74786840e-02 9.01329145e-03 0.00000000e+00 0.00000000e+00\n 9.29825939e-03 2.60619577e-02 1.06345778e-02 4.91392380e-03\n 1.06086470e-01 0.00000000e+00 1.77347916e-03 0.00000000e+00\n 0.00000000e+00 1.05241723e-01 0.00000000e+00 0.00000000e+00\n 9.81647670e-02 7.18060788e-03 0.00000000e+00 1.99685469e-02\n 1.04674347e-01 0.00000000e+00 0.00000000e+00 2.08112225e-01\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 7.83631112e-03 0.00000000e+00 0.00000000e+00\n 2.29869746e-02 1.16129611e-02 7.70034939e-02 0.00000000e+00\n 0.00000000e+00 9.80670657e-03 0.00000000e+00 8.31813377e-04\n 0.00000000e+00 3.28001641e-02 7.92734418e-03 0.00000000e+00\n 4.81772274e-02 7.63715804e-02 6.21853098e-02 7.07589090e-02].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "test22 = stdSlr.transform(test22)\n",
    "test22 = np.reshape(test22, (1, -1))\n",
    "test22.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
