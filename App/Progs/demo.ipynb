{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from pandas import DataFrame\n",
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import operator\n",
    "import os\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from itertools import chain\n",
    "import mahotas as mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'C:/Users/Joe/Desktop/UNI/Yr3/Dissertation/Datasets/101_ObjectCategories'\n",
    "folder_names = []\n",
    "folder_names = [f for f in sorted(os.listdir(file_path))]\n",
    "\n",
    "img_data = []\n",
    "img_classes = []\n",
    "\n",
    "num_categories = 8\n",
    "num_imgs = 30\n",
    "selected_categories = np.random.randint(0, 101, num_categories, dtype=\"I\")\n",
    "\n",
    "for a, cat in enumerate(selected_categories):\n",
    "    folder_path = file_path + '/' + folder_names[cat]\n",
    "    image_names = [a for a in sorted(\n",
    "        os.listdir(folder_path))][:num_imgs]\n",
    "\n",
    "    for b, img_name in enumerate(image_names):\n",
    "        img_classes.append(folder_names[cat])\n",
    "        img_path = folder_path + '/' + img_name\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.resize(img, (100, 100))\n",
    "\n",
    "        if img is not None:\n",
    "            img_data.append(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[1.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         1.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.00122126 0.00122126 0.00366379\n 0.00366379 0.00488505 0.00732758 0.00610631 0.00488505 0.0097701\n 0.0195402  0.00732758 0.01465515 0.01465515 0.01343389 0.01709767\n 0.01831894 0.02198273 0.02442525 0.01587641 0.01343389 0.01587641\n 0.02198273 0.03541661 0.01831894 0.02564651 0.03297409 0.03541661\n 0.03419535 0.03419535 0.04396545 0.05129303 0.03541661 0.03663788\n 0.04640798 0.05251429 0.0390804  0.05984186 0.05251429 0.03541661\n 0.0488505  0.04640798 0.03785914 0.04152292 0.04518671 0.04152292\n 0.04762924 0.04396545 0.06350565 0.05129303 0.05129303 0.05984186\n 0.0586206  0.06594817 0.0586206  0.04640798 0.05251429 0.05007176\n 0.05495681 0.05495681 0.06961196 0.06472691 0.07205449 0.06716944\n 0.08182459 0.07327575 0.08304585 0.07083323 0.07327575 0.0683907\n 0.06594817 0.08304585 0.06350565 0.09037343 0.06716944 0.08060332\n 0.08548837 0.08060332 0.08060332 0.09892226 0.09525847 0.1074711\n 0.09647974 0.08670964 0.08426711 0.12212625 0.10502858 0.10869236\n 0.1172412  0.11479867 0.09892226 0.10991362 0.12579004 0.12945382\n 0.1465515  0.16242792 0.17464054 0.16364917 0.18318938 0.195402\n 0.18318938 0.17708306 0.1758618  0.1856319  0.15754287 0.1270113\n 0.12334751 0.15021528 0.1172412  0.10380732 0.097701   0.10136478\n 0.08426711 0.10380732 0.09159469 0.08182459 0.08182459 0.08548837\n 0.09037343 0.07205449 0.08182459 0.07205449 0.05984186 0.08548837\n 0.07205449 0.07693954 0.08426711 0.0683907  0.06961196 0.05373555\n 0.05373555 0.05129303 0.04518671 0.03419535 0.03419535 0.05007176\n 0.04152292 0.02442525 0.05251429 0.03053156 0.03541661 0.03785914\n 0.03419535 0.03175282 0.03419535 0.03663788 0.02808904 0.02564651\n 0.0293103  0.02808904 0.02686778 0.0293103  0.03419535 0.02320399\n 0.03297409 0.02564651 0.02442525 0.0293103  0.03541661 0.03419535\n 0.03541661 0.02808904 0.03663788 0.04518671 0.03785914 0.02808904\n 0.02808904 0.02808904 0.03785914 0.03541661 0.0195402  0.02442525\n 0.03053156 0.02808904 0.01831894 0.03419535 0.0293103  0.02076146\n 0.02442525 0.01099136 0.02442525 0.02320399 0.02442525 0.0195402\n 0.02320399 0.02442525 0.02808904 0.02564651 0.02564651 0.02320399\n 0.01709767 0.02808904 0.01587641 0.02686778 0.03053156 0.02564651\n 0.02442525 0.02198273 0.02686778 0.04030166 0.02564651 0.02076146\n 0.02076146 0.02442525 0.03175282 0.02564651 0.01831894 0.0195402\n 0.0293103  0.02564651 0.01831894 0.02320399 0.01831894 0.01465515\n 0.01587641 0.02442525 0.02564651 0.02808904 0.01587641 0.01465515\n 0.01709767 0.02320399 0.0195402  0.01099136 0.01709767 0.01099136\n 0.02076146 0.0195402  0.02320399 0.01709767 0.02076146 0.01465515\n 0.00732758 0.02076146 0.02686778 0.02198273 0.02198273 0.01465515\n 0.0097701  0.01709767 0.01587641 0.01831894 0.02076146 0.01709767\n 0.02198273 0.01465515 0.01343389 0.02076146 0.02686778 0.01587641].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [4], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mM2\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mM2\u001b[39;00m\n\u001b[0;32m      3\u001b[0m clf \u001b[39m=\u001b[39m M2\u001b[39m.\u001b[39mload_svm()\n\u001b[1;32m----> 4\u001b[0m test_f \u001b[39m=\u001b[39m M2\u001b[39m.\u001b[39;49mprocess_query_M2(img_data[\u001b[39m22\u001b[39;49m])\n",
      "File \u001b[1;32mc:\\Users\\Joe\\Desktop\\UNI\\Yr3\\Dissertation\\System\\App\\Progs\\M2.py:272\u001b[0m, in \u001b[0;36mprocess_query_M2\u001b[1;34m(query_img, slice_n, type_h, norm)\u001b[0m\n\u001b[0;32m    269\u001b[0m scaler \u001b[39m=\u001b[39m MinMaxScaler()\n\u001b[0;32m    271\u001b[0m colour_f \u001b[39m=\u001b[39m colour_hist(query_img, \u001b[39m'\u001b[39m\u001b[39mglobal\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m12\u001b[39m, \u001b[39m3\u001b[39m, norm)\n\u001b[1;32m--> 272\u001b[0m colour_f \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39;49mfit(colour_f)\n\u001b[0;32m    274\u001b[0m \u001b[39m# texture features\u001b[39;00m\n\u001b[0;32m    275\u001b[0m gabor_kernals \u001b[39m=\u001b[39m build_gabor_kernals(\u001b[39m4\u001b[39m, (\u001b[39m0.1\u001b[39m, \u001b[39m0.5\u001b[39m, \u001b[39m0.8\u001b[39m), (\u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m))\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:420\u001b[0m, in \u001b[0;36mMinMaxScaler.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    418\u001b[0m \u001b[39m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[1;32m--> 420\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpartial_fit(X, y)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:457\u001b[0m, in \u001b[0;36mMinMaxScaler.partial_fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    452\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMinMaxScaler does not support sparse input. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    453\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConsider using MaxAbsScaler instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    454\u001b[0m     )\n\u001b[0;32m    456\u001b[0m first_pass \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mn_samples_seen_\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 457\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    458\u001b[0m     X,\n\u001b[0;32m    459\u001b[0m     reset\u001b[39m=\u001b[39;49mfirst_pass,\n\u001b[0;32m    460\u001b[0m     dtype\u001b[39m=\u001b[39;49mFLOAT_DTYPES,\n\u001b[0;32m    461\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    462\u001b[0m )\n\u001b[0;32m    464\u001b[0m data_min \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mnanmin(X, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m    465\u001b[0m data_max \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mnanmax(X, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\sklearn\\base.py:577\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    575\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    576\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 577\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    578\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[0;32m    579\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:879\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    877\u001b[0m     \u001b[39m# If input is 1D raise error\u001b[39;00m\n\u001b[0;32m    878\u001b[0m     \u001b[39mif\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 879\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    880\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mExpected 2D array, got 1D array instead:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39marray=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    881\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    882\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mif it contains a single sample.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[0;32m    884\u001b[0m         )\n\u001b[0;32m    886\u001b[0m \u001b[39mif\u001b[39;00m dtype_numeric \u001b[39mand\u001b[39;00m array\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mkind \u001b[39min\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mUSV\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    887\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    888\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdtype=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnumeric\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    889\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    890\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[1.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         1.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.00122126 0.00122126 0.00366379\n 0.00366379 0.00488505 0.00732758 0.00610631 0.00488505 0.0097701\n 0.0195402  0.00732758 0.01465515 0.01465515 0.01343389 0.01709767\n 0.01831894 0.02198273 0.02442525 0.01587641 0.01343389 0.01587641\n 0.02198273 0.03541661 0.01831894 0.02564651 0.03297409 0.03541661\n 0.03419535 0.03419535 0.04396545 0.05129303 0.03541661 0.03663788\n 0.04640798 0.05251429 0.0390804  0.05984186 0.05251429 0.03541661\n 0.0488505  0.04640798 0.03785914 0.04152292 0.04518671 0.04152292\n 0.04762924 0.04396545 0.06350565 0.05129303 0.05129303 0.05984186\n 0.0586206  0.06594817 0.0586206  0.04640798 0.05251429 0.05007176\n 0.05495681 0.05495681 0.06961196 0.06472691 0.07205449 0.06716944\n 0.08182459 0.07327575 0.08304585 0.07083323 0.07327575 0.0683907\n 0.06594817 0.08304585 0.06350565 0.09037343 0.06716944 0.08060332\n 0.08548837 0.08060332 0.08060332 0.09892226 0.09525847 0.1074711\n 0.09647974 0.08670964 0.08426711 0.12212625 0.10502858 0.10869236\n 0.1172412  0.11479867 0.09892226 0.10991362 0.12579004 0.12945382\n 0.1465515  0.16242792 0.17464054 0.16364917 0.18318938 0.195402\n 0.18318938 0.17708306 0.1758618  0.1856319  0.15754287 0.1270113\n 0.12334751 0.15021528 0.1172412  0.10380732 0.097701   0.10136478\n 0.08426711 0.10380732 0.09159469 0.08182459 0.08182459 0.08548837\n 0.09037343 0.07205449 0.08182459 0.07205449 0.05984186 0.08548837\n 0.07205449 0.07693954 0.08426711 0.0683907  0.06961196 0.05373555\n 0.05373555 0.05129303 0.04518671 0.03419535 0.03419535 0.05007176\n 0.04152292 0.02442525 0.05251429 0.03053156 0.03541661 0.03785914\n 0.03419535 0.03175282 0.03419535 0.03663788 0.02808904 0.02564651\n 0.0293103  0.02808904 0.02686778 0.0293103  0.03419535 0.02320399\n 0.03297409 0.02564651 0.02442525 0.0293103  0.03541661 0.03419535\n 0.03541661 0.02808904 0.03663788 0.04518671 0.03785914 0.02808904\n 0.02808904 0.02808904 0.03785914 0.03541661 0.0195402  0.02442525\n 0.03053156 0.02808904 0.01831894 0.03419535 0.0293103  0.02076146\n 0.02442525 0.01099136 0.02442525 0.02320399 0.02442525 0.0195402\n 0.02320399 0.02442525 0.02808904 0.02564651 0.02564651 0.02320399\n 0.01709767 0.02808904 0.01587641 0.02686778 0.03053156 0.02564651\n 0.02442525 0.02198273 0.02686778 0.04030166 0.02564651 0.02076146\n 0.02076146 0.02442525 0.03175282 0.02564651 0.01831894 0.0195402\n 0.0293103  0.02564651 0.01831894 0.02320399 0.01831894 0.01465515\n 0.01587641 0.02442525 0.02564651 0.02808904 0.01587641 0.01465515\n 0.01709767 0.02320399 0.0195402  0.01099136 0.01709767 0.01099136\n 0.02076146 0.0195402  0.02320399 0.01709767 0.02076146 0.01465515\n 0.00732758 0.02076146 0.02686778 0.02198273 0.02198273 0.01465515\n 0.0097701  0.01709767 0.01587641 0.01831894 0.02076146 0.01709767\n 0.02198273 0.01465515 0.01343389 0.02076146 0.02686778 0.01587641].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "import M2 as M2\n",
    "\n",
    "clf = M2.load_svm()\n",
    "test_f = M2.process_query_M2(img_data[22])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
